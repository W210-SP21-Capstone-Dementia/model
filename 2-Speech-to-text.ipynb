{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.0.1)\n",
      "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.6/dist-packages (3.8.1)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.24.1)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install --upgrade pip\n",
    "! python3 -m pip install  --upgrade SpeechRecognition\n",
    "! python3 -m pip install  --upgrade pydub\n",
    "! python3 -m pip install  --upgrade sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import kapre\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import speech_recognition as sr\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import path\n",
    "from plotnine import *\n",
    "from pydub import AudioSegment\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe audio data to txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make an input from user\n",
    "data_path = '/tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train'\n",
    "\n",
    "audio_path_cc = data_path + '/Full_wave_enhanced_audio/cc/'\n",
    "audio_path_cd = data_path + '/Full_wave_enhanced_audio/cd/'\n",
    "\n",
    "\n",
    "text_path_cc = data_path + '/transcription/cc/'\n",
    "text_path_cd = data_path + '/transcription/cd/'\n",
    "\n",
    "lang_ = 'en-US'\n",
    "\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>NA</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S005</td>\n",
       "      <td>74</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender MMSE Group\n",
       "0        0  S001   74     male    NA    cc\n",
       "1        1  S002   62   female    30    cc\n",
       "2        2  S003   69   female    29    cc\n",
       "3        3  S004   71   female    30    cc\n",
       "4        4  S005   74   female    30    cc\n",
       "..     ...   ...  ...       ...  ...   ...\n",
       "103     49  S150   58     male    20    cd\n",
       "104     50  S151   72     male    24    cd\n",
       "105     51  S153   68   female    12    cd\n",
       "106     52  S154   65   female    20    cd\n",
       "107     53  S156   71   female    13    cd\n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_cc = pd.read_csv(data_path + '/cc_meta_data.txt', sep=\";\", header=0, \n",
    "                      names = ['ID', 'Age', 'Gender', 'MMSE'])\n",
    "meta_cd = pd.read_csv(data_path + '/cd_meta_data.txt', sep=\";\", header=0, \n",
    "                      names = ['ID', 'Age', 'Gender', 'MMSE'])\n",
    "\n",
    "meta = meta_cc.assign(Group = 'cc').append(meta_cd.assign(Group = 'cd')).reset_index()\n",
    "\n",
    "meta['ID'] = meta['ID'].str.strip()\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(meta.MMSE == ' NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S005</td>\n",
       "      <td>74</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender  MMSE Group\n",
       "0        0  S001   74     male     30    cc\n",
       "1        1  S002   62   female     30    cc\n",
       "2        2  S003   69   female     29    cc\n",
       "3        3  S004   71   female     30    cc\n",
       "4        4  S005   74   female     30    cc\n",
       "..     ...   ...  ...       ...   ...   ...\n",
       "103     49  S150   58     male     20    cd\n",
       "104     50  S151   72     male     24    cd\n",
       "105     51  S153   68   female     12    cd\n",
       "106     52  S154   65   female     20    cd\n",
       "107     53  S156   71   female     13    cd\n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.MMSE = pd.to_numeric(meta.MMSE.replace(' NA', 30))\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      int64\n",
       "ID        object\n",
       "Age        int64\n",
       "Gender    object\n",
       "MMSE       int64\n",
       "Group     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startConversion(path, filename, lang = lang_):\n",
    "                \n",
    "    # Create output file name\n",
    "    output_dir = path + 'output_text/'\n",
    "    output_file = '.'.join(filename.split(sep='.')[:-1]) + '.txt'\n",
    "    output_file_path = output_dir + output_file\n",
    "    \n",
    "    # If output file does not exist, continue\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(\"Sorry, \" + output_file_path + \" already exists\")\n",
    "    else:\n",
    "        full_path = path + filename\n",
    "\n",
    "        with sr.AudioFile(full_path) as source:\n",
    "            print('Transcribing file: ' + str(filename) + ' in path: ' + str(full_path))\n",
    "            audio_text = r.listen(source)\n",
    "            # recognize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "            try:\n",
    "\n",
    "                # using google speech recognition\n",
    "                # print('Converting audio transcripts into text ...')\n",
    "                text = r.recognize_google(audio_text)\n",
    "\n",
    "                # Create output directory\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "\n",
    "                with open(output_file_path, 'w') as f:\n",
    "                    f.write(text)\n",
    "                print('Finished transcribing text file ' + str(output_file) + ' at location ' + output_file_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error: ' + str(e) + ' <- this guy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S001.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S002.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S003.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S004.txt already exists\n",
      "Transcribing file: S005.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S005.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S006.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S006.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S007.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S007.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S009.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S011.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S012.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S013.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S015.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S016.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S017.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S018.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S019.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S020.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S021.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S024.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S025.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S027.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S028.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S029.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S030.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S032.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S033.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S034.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S035.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S036.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S038.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S039.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S040.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S041.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S043.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S048.txt already exists\n",
      "Transcribing file: S049.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S049.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S051.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S052.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S055.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S056.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S058.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S059.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S061.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S062.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S063.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S064.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S067.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S068.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S070.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S071.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S072.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S073.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S076.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S077.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Running for CC path\n",
    "onlyfiles = [f for f in listdir(audio_path_cc) if isfile(join(audio_path_cc, f))]\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    startConversion(path = audio_path_cc, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S079.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S080.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S081.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S082.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S083.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S084.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S086.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S087.txt already exists\n",
      "Transcribing file: S089.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S089.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S090.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S092.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S093.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S094.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S095.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S096.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S097.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S100.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S101.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S103.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S104.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S107.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S108.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S110.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S111.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S114.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S116.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S118.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S122.txt already exists\n",
      "Transcribing file: S124.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S124.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S125.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S125.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S126.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S127.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S128.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S129.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S130.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S132.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S135.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S136.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S137.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S138.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S139.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S140.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S141.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S142.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S143.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S144.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S145.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S148.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S149.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S150.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S151.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S153.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S154.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S156.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Running for CD path\n",
    "onlyfiles = [f for f in listdir(audio_path_cd) if isfile(join(audio_path_cd, f))]\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    startConversion(path = audio_path_cd, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding to dictionary: S001\n",
      "Adding to dictionary: S002\n",
      "Adding to dictionary: S003\n",
      "Adding to dictionary: S004\n",
      "Adding to dictionary: S009\n",
      "Adding to dictionary: S011\n",
      "Adding to dictionary: S012\n",
      "Adding to dictionary: S013\n",
      "Adding to dictionary: S015\n",
      "Adding to dictionary: S016\n",
      "Adding to dictionary: S017\n",
      "Adding to dictionary: S018\n",
      "Adding to dictionary: S019\n",
      "Adding to dictionary: S020\n",
      "Adding to dictionary: S021\n",
      "Adding to dictionary: S024\n",
      "Adding to dictionary: S025\n",
      "Adding to dictionary: S027\n",
      "Adding to dictionary: S028\n",
      "Adding to dictionary: S029\n",
      "Adding to dictionary: S030\n",
      "Adding to dictionary: S032\n",
      "Adding to dictionary: S033\n",
      "Adding to dictionary: S034\n",
      "Adding to dictionary: S035\n",
      "Adding to dictionary: S036\n",
      "Adding to dictionary: S038\n",
      "Adding to dictionary: S039\n",
      "Adding to dictionary: S040\n",
      "Adding to dictionary: S041\n",
      "Adding to dictionary: S043\n",
      "Adding to dictionary: S048\n",
      "Adding to dictionary: S051\n",
      "Adding to dictionary: S052\n",
      "Adding to dictionary: S055\n",
      "Adding to dictionary: S056\n",
      "Adding to dictionary: S058\n",
      "Adding to dictionary: S059\n",
      "Adding to dictionary: S061\n",
      "Adding to dictionary: S062\n",
      "Adding to dictionary: S063\n",
      "Adding to dictionary: S064\n",
      "Adding to dictionary: S067\n",
      "Adding to dictionary: S068\n",
      "Adding to dictionary: S070\n",
      "Adding to dictionary: S071\n",
      "Adding to dictionary: S072\n",
      "Adding to dictionary: S073\n",
      "Adding to dictionary: S076\n",
      "Adding to dictionary: S077\n",
      "Adding to dictionary: S079\n",
      "Adding to dictionary: S080\n",
      "Adding to dictionary: S081\n",
      "Adding to dictionary: S082\n",
      "Adding to dictionary: S083\n",
      "Adding to dictionary: S084\n",
      "Adding to dictionary: S086\n",
      "Adding to dictionary: S087\n",
      "Adding to dictionary: S090\n",
      "Adding to dictionary: S092\n",
      "Adding to dictionary: S093\n",
      "Adding to dictionary: S094\n",
      "Adding to dictionary: S095\n",
      "Adding to dictionary: S096\n",
      "Adding to dictionary: S097\n",
      "Adding to dictionary: S100\n",
      "Adding to dictionary: S101\n",
      "Adding to dictionary: S103\n",
      "Adding to dictionary: S104\n",
      "Adding to dictionary: S107\n",
      "Adding to dictionary: S108\n",
      "Adding to dictionary: S110\n",
      "Adding to dictionary: S111\n",
      "Adding to dictionary: S114\n",
      "Adding to dictionary: S116\n",
      "Adding to dictionary: S118\n",
      "Adding to dictionary: S122\n",
      "Adding to dictionary: S126\n",
      "Adding to dictionary: S127\n",
      "Adding to dictionary: S128\n",
      "Adding to dictionary: S129\n",
      "Adding to dictionary: S130\n",
      "Adding to dictionary: S132\n",
      "Adding to dictionary: S135\n",
      "Adding to dictionary: S136\n",
      "Adding to dictionary: S137\n",
      "Adding to dictionary: S138\n",
      "Adding to dictionary: S139\n",
      "Adding to dictionary: S140\n",
      "Adding to dictionary: S141\n",
      "Adding to dictionary: S142\n",
      "Adding to dictionary: S143\n",
      "Adding to dictionary: S144\n",
      "Adding to dictionary: S145\n",
      "Adding to dictionary: S148\n",
      "Adding to dictionary: S149\n",
      "Adding to dictionary: S150\n",
      "Adding to dictionary: S151\n",
      "Adding to dictionary: S153\n",
      "Adding to dictionary: S154\n",
      "Adding to dictionary: S156\n",
      "Adding to dictionary: wav\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "text_dict = {\"ID\": [], \"Text\": []}\n",
    "\n",
    "# CC path\n",
    "onlytextfiles_cc = [f for f in listdir(audio_cc_text_path) if isfile(join(audio_cc_text_path, f))]\n",
    "        \n",
    "for filename in onlytextfiles_cc:\n",
    "    just_name = filename.split(sep='.')[:-1][0]\n",
    "    # print(\"Adding to dictionary: \" + str(just_name))\n",
    "    full_text_file_path = audio_cc_text_path + filename\n",
    "\n",
    "    # Make all text lowercase\n",
    "    # Remove special characters\n",
    "    with open(full_text_file_path, \"r\") as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        text_dict[\"ID\"].append(just_name)\n",
    "        text_dict[\"Text\"].append(data)\n",
    "\n",
    "# CD path\n",
    "onlytextfiles_cd = [f for f in listdir(audio_cd_text_path) if isfile(join(audio_cd_text_path, f))]\n",
    "\n",
    "for filename in onlytextfiles_cd:\n",
    "    just_name = filename.split(sep='.')[:-1][0]\n",
    "    # print(\"Adding to dictionary: \" + just_name)\n",
    "    full_text_file_path = audio_cd_text_path + filename\n",
    "\n",
    "    # Make all text lowercase\n",
    "    # Remove special characters\n",
    "    with open(full_text_file_path, \"r\") as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        text_dict[\"ID\"].append(just_name)\n",
    "        text_dict[\"Text\"].append(data)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S009</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>S151</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>S153</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>S154</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>S156</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>wav</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text\n",
       "0    S001  tell me everything that you see going on in th...\n",
       "1    S002                                            picture\n",
       "2    S003  okay there is a little boy and he's getting he...\n",
       "3    S004  Homedics laugh you ready well the sink is over...\n",
       "4    S009  boy is taking cookies from the cookie jar givi...\n",
       "..    ...                                                ...\n",
       "97   S151  everything that you see happening in that pict...\n",
       "98   S153  and tell me everything that you see happening ...\n",
       "99   S154  okay and the boys getting in the cookie jar is...\n",
       "100  S156  can you tell me now this one is in the cookie ...\n",
       "101   wav  can you tell me now this one is in the cookie ...\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dictionary into DataFrame \n",
    "text_df = pd.DataFrame(text_dict)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>cc</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>S009</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "      <td>now the boy on the chair is falling reaching u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender  MMSE Group  \\\n",
       "0        0  S001   74     male     30    cc   \n",
       "1        1  S002   62   female     30    cc   \n",
       "2        2  S003   69   female     29    cc   \n",
       "3        3  S004   71   female     30    cc   \n",
       "4        7  S009   67     male     30    cc   \n",
       "..     ...   ...  ...       ...   ...   ...   \n",
       "96      49  S150   58     male     20    cd   \n",
       "97      50  S151   72     male     24    cd   \n",
       "98      51  S153   68   female     12    cd   \n",
       "99      52  S154   65   female     20    cd   \n",
       "100     53  S156   71   female     13    cd   \n",
       "\n",
       "                                                  Text  \n",
       "0    tell me everything that you see going on in th...  \n",
       "1                                              picture  \n",
       "2    okay there is a little boy and he's getting he...  \n",
       "3    Homedics laugh you ready well the sink is over...  \n",
       "4    boy is taking cookies from the cookie jar givi...  \n",
       "..                                                 ...  \n",
       "96   now the boy on the chair is falling reaching u...  \n",
       "97   everything that you see happening in that pict...  \n",
       "98   and tell me everything that you see happening ...  \n",
       "99   okay and the boys getting in the cookie jar is...  \n",
       "100  can you tell me now this one is in the cookie ...  \n",
       "\n",
       "[101 rows x 7 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.merge(meta, text_df, on = \"ID\", how = \"inner\")\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set is length: 80\n",
      "Test data set is length: 21\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text data so model can take it in\n",
    "# Convert to 'dense array' (NP array)\n",
    "# TODO make it so english is not hardcoded, flexible for all languages\n",
    "# vectorizer = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1, 1))\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(merged_data[\"Text\"]).toarray().astype(np.int)\n",
    "y = np.array(merged_data[\"MMSE\"].values).astype(np.int)\n",
    "\n",
    "# dividing X, y into train and test data \n",
    "# 80/20 training/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"Training data set is length: \" +str(len(train)))\n",
    "print(\"Test data set is length: \" +str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and predict on Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 131 features per sample; expecting 393",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-710d63149668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use score method to get accuracy of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogisticRegr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 273\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 131 features per sample; expecting 393"
     ]
    }
   ],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 131 should be equal to 343, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-6b038fb4a361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training a linear SVM classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvm_model_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvm_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_model_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model accuracy for X_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    465\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[1;32m    466\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 131 should be equal to 343, the number of features at training time"
     ]
    }
   ],
   "source": [
    "# training a linear SVM classifier \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train_vectorized, y_train) \n",
    "svm_predictions = svm_model_linear.predict(X_test_vectorized) \n",
    "  \n",
    "# model accuracy for X_test   \n",
    "accuracy = svm_model_linear.score(X_test_vectorized, y_test) \n",
    "  \n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(y_test, svm_predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - predict on test data (needs to be labelled?) and score\n",
    "# But then it would be a supervised learning problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO, try K means and other multi-class classification as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete test and training data so we can get future random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
