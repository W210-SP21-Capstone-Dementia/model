{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "Successfully installed pip-21.0.1\n",
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 32.8 MB 29 kB/s  eta 0:00:01     |███████████▉                    | 12.2 MB 6.5 MB/s eta 0:00:04     |███████████████████████▊        | 24.3 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.8.1\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.24.1-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.24.1\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=f712180d1822d3f130f14b0b595de718a95ae41be780e242d7acf986acaca64c\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install --upgrade pip\n",
    "! python3 -m pip install  --upgrade SpeechRecognition\n",
    "! python3 -m pip install  --upgrade pydub\n",
    "! python3 -m pip install  --upgrade sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import kapre\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import speech_recognition as sr\n",
    "import tensorflow as tf\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import path\n",
    "from plotnine import *\n",
    "from pydub import AudioSegment\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe audio data to txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make an input from user\n",
    "data_path = '/tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train'\n",
    "\n",
    "audio_path_cc = data_path + '/Full_wave_enhanced_audio/cc/'\n",
    "audio_path_cd = data_path + '/Full_wave_enhanced_audio/cd/'\n",
    "\n",
    "\n",
    "text_path_cc = data_path + '/transcription/cc/'\n",
    "text_path_cd = data_path + '/transcription/cd/'\n",
    "\n",
    "lang_ = 'en-US'\n",
    "\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>NA</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S005</td>\n",
       "      <td>74</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender MMSE Group\n",
       "0        0  S001   74     male    NA    cc\n",
       "1        1  S002   62   female    30    cc\n",
       "2        2  S003   69   female    29    cc\n",
       "3        3  S004   71   female    30    cc\n",
       "4        4  S005   74   female    30    cc\n",
       "..     ...   ...  ...       ...  ...   ...\n",
       "103     49  S150   58     male    20    cd\n",
       "104     50  S151   72     male    24    cd\n",
       "105     51  S153   68   female    12    cd\n",
       "106     52  S154   65   female    20    cd\n",
       "107     53  S156   71   female    13    cd\n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_cc = pd.read_csv(data_path + '/cc_meta_data.txt', sep=\";\", header=0, \n",
    "                      names = ['ID', 'Age', 'Gender', 'MMSE'])\n",
    "meta_cd = pd.read_csv(data_path + '/cd_meta_data.txt', sep=\";\", header=0, \n",
    "                      names = ['ID', 'Age', 'Gender', 'MMSE'])\n",
    "\n",
    "meta = meta_cc.assign(Group = 'cc').append(meta_cd.assign(Group = 'cd')).reset_index()\n",
    "\n",
    "meta['ID'] = meta['ID'].str.strip()\n",
    "\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(meta.MMSE == ' NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S005</td>\n",
       "      <td>74</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender  MMSE Group\n",
       "0        0  S001   74     male     30    cc\n",
       "1        1  S002   62   female     30    cc\n",
       "2        2  S003   69   female     30    cc\n",
       "3        3  S004   71   female     30    cc\n",
       "4        4  S005   74   female     30    cc\n",
       "..     ...   ...  ...       ...   ...   ...\n",
       "103     49  S150   58     male     20    cd\n",
       "104     50  S151   72     male     24    cd\n",
       "105     51  S153   68   female     12    cd\n",
       "106     52  S154   65   female     20    cd\n",
       "107     53  S156   71   female     13    cd\n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all NAs\n",
    "meta.MMSE = pd.to_numeric(meta.MMSE.replace(' NA', 30))\n",
    "\n",
    "# TODO - see if this works - Replace all scores in cc group to 30\n",
    "meta.loc[meta['Group'] == 'cc', 'MMSE'] = 30\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index      int64\n",
       "ID        object\n",
       "Age        int64\n",
       "Gender    object\n",
       "MMSE       int64\n",
       "Group     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startConversion(path, filename, lang = lang_):\n",
    "                \n",
    "    # Create output file name\n",
    "    output_dir = path + 'output_text/'\n",
    "    output_file = '.'.join(filename.split(sep='.')[:-1]) + '.txt'\n",
    "    output_file_path = output_dir + output_file\n",
    "    \n",
    "    # If output file does not exist, continue\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(\"Sorry, \" + output_file_path + \" already exists\")\n",
    "    else:\n",
    "        full_path = path + filename\n",
    "\n",
    "        with sr.AudioFile(full_path) as source:\n",
    "            print('Transcribing file: ' + str(filename) + ' in path: ' + str(full_path))\n",
    "            audio_text = r.listen(source)\n",
    "            # recognize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
    "            try:\n",
    "\n",
    "                # using google speech recognition\n",
    "                # print('Converting audio transcripts into text ...')\n",
    "                text = r.recognize_google(audio_text)\n",
    "\n",
    "                # Create output directory\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "\n",
    "                with open(output_file_path, 'w') as f:\n",
    "                    f.write(text)\n",
    "                print('Finished transcribing text file ' + str(output_file) + ' at location ' + output_file_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error: ' + str(e) + ' <- this guy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S001.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S002.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S003.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S004.txt already exists\n",
      "Transcribing file: S005.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S005.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S006.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S006.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S007.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S007.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S009.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S011.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S012.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S013.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S015.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S016.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S017.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S018.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S019.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S020.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S021.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S024.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S025.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S027.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S028.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S029.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S030.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S032.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S033.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S034.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S035.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S036.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S038.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S039.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S040.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S041.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S043.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S048.txt already exists\n",
      "Transcribing file: S049.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/S049.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S051.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S052.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S055.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S056.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S058.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S059.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S061.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S062.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S063.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S064.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S067.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S068.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S070.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S071.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S072.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S073.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S076.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cc/output_text/S077.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Running for CC path\n",
    "onlyfiles = [f for f in listdir(audio_path_cc) if isfile(join(audio_path_cc, f))]\n",
    "\n",
    "# Creating this for reference later\n",
    "audio_cc_text_path = audio_path_cc + 'output_text/'\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    startConversion(path = audio_path_cc, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S079.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S080.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S081.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S082.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S083.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S084.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S086.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S087.txt already exists\n",
      "Transcribing file: S089.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S089.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S090.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S092.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S093.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S094.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S095.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S096.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S097.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S100.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S101.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S103.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S104.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S107.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S108.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S110.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S111.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S114.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S116.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S118.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S122.txt already exists\n",
      "Transcribing file: S124.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S124.wav\n",
      "Error:  <- this guy\n",
      "Transcribing file: S125.wav in path: /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/S125.wav\n",
      "Error:  <- this guy\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S126.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S127.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S128.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S129.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S130.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S132.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S135.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S136.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S137.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S138.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S139.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S140.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S141.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S142.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S143.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S144.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S145.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S148.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S149.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S150.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S151.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S153.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S154.txt already exists\n",
      "Sorry, /tf/dementia/0extra/ADReSS-IS2020-train/ADReSS-IS2020-data/train/Full_wave_enhanced_audio/cd/output_text/S156.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Running for CD path\n",
    "onlyfiles = [f for f in listdir(audio_path_cd) if isfile(join(audio_path_cd, f))]\n",
    "\n",
    "# Creating this for reference later\n",
    "audio_cd_text_path = audio_path_cd + 'output_text/'\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    startConversion(path = audio_path_cd, filename = filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "text_dict = {\"ID\": [], \"Text\": []}\n",
    "\n",
    "# CC path\n",
    "onlytextfiles_cc = [f for f in listdir(audio_cc_text_path) if isfile(join(audio_cc_text_path, f))]\n",
    "        \n",
    "for filename in onlytextfiles_cc:\n",
    "    just_name = filename.split(sep='.')[:-1][0]\n",
    "    # print(\"Adding to dictionary: \" + str(just_name))\n",
    "    full_text_file_path = audio_cc_text_path + filename\n",
    "\n",
    "    # Make all text lowercase\n",
    "    # Remove special characters\n",
    "    with open(full_text_file_path, \"r\") as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        text_dict[\"ID\"].append(just_name)\n",
    "        text_dict[\"Text\"].append(data)\n",
    "\n",
    "# CD path\n",
    "onlytextfiles_cd = [f for f in listdir(audio_cd_text_path) if isfile(join(audio_cd_text_path, f))]\n",
    "\n",
    "for filename in onlytextfiles_cd:\n",
    "    just_name = filename.split(sep='.')[:-1][0]\n",
    "    # print(\"Adding to dictionary: \" + just_name)\n",
    "    full_text_file_path = audio_cd_text_path + filename\n",
    "\n",
    "    # Make all text lowercase\n",
    "    # Remove special characters\n",
    "    with open(full_text_file_path, \"r\") as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "        text_dict[\"ID\"].append(just_name)\n",
    "        text_dict[\"Text\"].append(data)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S001</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S002</td>\n",
       "      <td>picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S003</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S004</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S009</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>S151</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>S153</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>S154</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>S156</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>wav</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                               Text\n",
       "0    S001  tell me everything that you see going on in th...\n",
       "1    S002                                            picture\n",
       "2    S003  okay there is a little boy and he's getting he...\n",
       "3    S004  Homedics laugh you ready well the sink is over...\n",
       "4    S009  boy is taking cookies from the cookie jar givi...\n",
       "..    ...                                                ...\n",
       "97   S151  everything that you see happening in that pict...\n",
       "98   S153  and tell me everything that you see happening ...\n",
       "99   S154  okay and the boys getting in the cookie jar is...\n",
       "100  S156  can you tell me now this one is in the cookie ...\n",
       "101   wav  can you tell me now this one is in the cookie ...\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dictionary into DataFrame \n",
    "text_df = pd.DataFrame(text_dict)\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Group</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>S009</td>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>30</td>\n",
       "      <td>cc</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "      <td>now the boy on the chair is falling reaching u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>cd</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>female</td>\n",
       "      <td>12</td>\n",
       "      <td>cd</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>female</td>\n",
       "      <td>20</td>\n",
       "      <td>cd</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>female</td>\n",
       "      <td>13</td>\n",
       "      <td>cd</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age    Gender  MMSE Group  \\\n",
       "0        0  S001   74     male     30    cc   \n",
       "1        1  S002   62   female     30    cc   \n",
       "2        2  S003   69   female     30    cc   \n",
       "3        3  S004   71   female     30    cc   \n",
       "4        7  S009   67     male     30    cc   \n",
       "..     ...   ...  ...       ...   ...   ...   \n",
       "96      49  S150   58     male     20    cd   \n",
       "97      50  S151   72     male     24    cd   \n",
       "98      51  S153   68   female     12    cd   \n",
       "99      52  S154   65   female     20    cd   \n",
       "100     53  S156   71   female     13    cd   \n",
       "\n",
       "                                                  Text  \n",
       "0    tell me everything that you see going on in th...  \n",
       "1                                              picture  \n",
       "2    okay there is a little boy and he's getting he...  \n",
       "3    Homedics laugh you ready well the sink is over...  \n",
       "4    boy is taking cookies from the cookie jar givi...  \n",
       "..                                                 ...  \n",
       "96   now the boy on the chair is falling reaching u...  \n",
       "97   everything that you see happening in that pict...  \n",
       "98   and tell me everything that you see happening ...  \n",
       "99   okay and the boys getting in the cookie jar is...  \n",
       "100  can you tell me now this one is in the cookie ...  \n",
       "\n",
       "[101 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data = pd.merge(meta, text_df, on = \"ID\", how = \"inner\")\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making some relevant columns categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gender_ male</th>\n",
       "      <th>Group_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>30</td>\n",
       "      <td>picture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>S009</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>now the boy on the chair is falling reaching u...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>24</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age  MMSE  \\\n",
       "0        0  S001   74    30   \n",
       "1        1  S002   62    30   \n",
       "2        2  S003   69    30   \n",
       "3        3  S004   71    30   \n",
       "4        7  S009   67    30   \n",
       "..     ...   ...  ...   ...   \n",
       "96      49  S150   58    20   \n",
       "97      50  S151   72    24   \n",
       "98      51  S153   68    12   \n",
       "99      52  S154   65    20   \n",
       "100     53  S156   71    13   \n",
       "\n",
       "                                                  Text  Gender_ male   \\\n",
       "0    tell me everything that you see going on in th...              1   \n",
       "1                                              picture              0   \n",
       "2    okay there is a little boy and he's getting he...              0   \n",
       "3    Homedics laugh you ready well the sink is over...              0   \n",
       "4    boy is taking cookies from the cookie jar givi...              1   \n",
       "..                                                 ...            ...   \n",
       "96   now the boy on the chair is falling reaching u...              1   \n",
       "97   everything that you see happening in that pict...              1   \n",
       "98   and tell me everything that you see happening ...              0   \n",
       "99   okay and the boys getting in the cookie jar is...              0   \n",
       "100  can you tell me now this one is in the cookie ...              0   \n",
       "\n",
       "     Group_cd  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "96          1  \n",
       "97          1  \n",
       "98          1  \n",
       "99          1  \n",
       "100         1  \n",
       "\n",
       "[101 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dummy categorical values and remove redundant first column\n",
    "merged_data = pd.get_dummies(merged_data, columns=['Gender', 'Group'], drop_first=True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce output labels by bucketizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pubmed.ncbi.nlm.nih.gov/16473978/\n",
    "MMSE ranges:\n",
    "* 30 for no\n",
    "* 26-29 for questionable\n",
    "* 21-25 for mild\n",
    "* 11-20 for moderate\n",
    "* 0-10 for severe dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default value\n",
    "merged_data['dementia_stage_num'] = 0\n",
    "merged_data['dementia_stage_text'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>Text</th>\n",
       "      <th>Gender_ male</th>\n",
       "      <th>Group_cd</th>\n",
       "      <th>dementia_stage_num</th>\n",
       "      <th>dementia_stage_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S001</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>tell me everything that you see going on in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S002</td>\n",
       "      <td>62</td>\n",
       "      <td>30</td>\n",
       "      <td>picture</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S003</td>\n",
       "      <td>69</td>\n",
       "      <td>30</td>\n",
       "      <td>okay there is a little boy and he's getting he...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S004</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>Homedics laugh you ready well the sink is over...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>S009</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>boy is taking cookies from the cookie jar givi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>49</td>\n",
       "      <td>S150</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>now the boy on the chair is falling reaching u...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate/Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50</td>\n",
       "      <td>S151</td>\n",
       "      <td>72</td>\n",
       "      <td>24</td>\n",
       "      <td>everything that you see happening in that pict...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>51</td>\n",
       "      <td>S153</td>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>and tell me everything that you see happening ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate/Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>52</td>\n",
       "      <td>S154</td>\n",
       "      <td>65</td>\n",
       "      <td>20</td>\n",
       "      <td>okay and the boys getting in the cookie jar is...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate/Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>53</td>\n",
       "      <td>S156</td>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>can you tell me now this one is in the cookie ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate/Severe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index    ID  Age  MMSE  \\\n",
       "0        0  S001   74    30   \n",
       "1        1  S002   62    30   \n",
       "2        2  S003   69    30   \n",
       "3        3  S004   71    30   \n",
       "4        7  S009   67    30   \n",
       "..     ...   ...  ...   ...   \n",
       "96      49  S150   58    20   \n",
       "97      50  S151   72    24   \n",
       "98      51  S153   68    12   \n",
       "99      52  S154   65    20   \n",
       "100     53  S156   71    13   \n",
       "\n",
       "                                                  Text  Gender_ male   \\\n",
       "0    tell me everything that you see going on in th...              1   \n",
       "1                                              picture              0   \n",
       "2    okay there is a little boy and he's getting he...              0   \n",
       "3    Homedics laugh you ready well the sink is over...              0   \n",
       "4    boy is taking cookies from the cookie jar givi...              1   \n",
       "..                                                 ...            ...   \n",
       "96   now the boy on the chair is falling reaching u...              1   \n",
       "97   everything that you see happening in that pict...              1   \n",
       "98   and tell me everything that you see happening ...              0   \n",
       "99   okay and the boys getting in the cookie jar is...              0   \n",
       "100  can you tell me now this one is in the cookie ...              0   \n",
       "\n",
       "     Group_cd  dementia_stage_num dementia_stage_text  \n",
       "0           0                   0                None  \n",
       "1           0                   0                None  \n",
       "2           0                   0                None  \n",
       "3           0                   0                None  \n",
       "4           0                   0                None  \n",
       "..        ...                 ...                 ...  \n",
       "96          1                   2     Moderate/Severe  \n",
       "97          1                   1                Mild  \n",
       "98          1                   2     Moderate/Severe  \n",
       "99          1                   2     Moderate/Severe  \n",
       "100         1                   2     Moderate/Severe  \n",
       "\n",
       "[101 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dementia stage value for all row indexes which MMSE are in certain ranges\n",
    "\n",
    "# Mild stage\n",
    "merged_data['dementia_stage_num'][(merged_data['MMSE'] > 20) & (merged_data['MMSE'] < 30)] = 1\n",
    "merged_data['dementia_stage_text'][(merged_data['MMSE'] > 20) & (merged_data['MMSE'] < 30)] = 'Mild'\n",
    "\n",
    "# Moderate/Severe stage\n",
    "merged_data['dementia_stage_num'][(merged_data['MMSE'] < 21)] = 2\n",
    "merged_data['dementia_stage_text'][(merged_data['MMSE'] < 21)] = 'Moderate/Severe'\n",
    "\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    51\n",
      "2    39\n",
      "1    11\n",
      "Name: dementia_stage_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# See number of each type\n",
    "print(merged_data['dementia_stage_num'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10  104  303   40    about  acting  action  active  ahead       all  \\\n",
      "0    0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "1    0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "2    0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.081485   \n",
      "3    0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "4    0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.171352   \n",
      "..   ...  ...  ...  ...      ...     ...     ...     ...    ...       ...   \n",
      "96   0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "97   0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "98   0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "99   0.0  0.0  0.0  0.0  0.17899     0.0     0.0     0.0    0.0  0.000000   \n",
      "100  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0    0.0  0.000000   \n",
      "\n",
      "     ...      with  woman   working  yard  year  yet       you  young  \\\n",
      "0    ...  0.208362    0.0  0.000000   0.0   0.0  0.0  0.164697    0.0   \n",
      "1    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
      "2    ...  0.274953    0.0  0.000000   0.0   0.0  0.0  0.048296    0.0   \n",
      "3    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.100045    0.0   \n",
      "4    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
      "..   ...       ...    ...       ...   ...   ...  ...       ...    ...   \n",
      "96   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
      "97   ...  0.000000    0.0  0.239497   0.0   0.0  0.0  0.241061    0.0   \n",
      "98   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.159917    0.0   \n",
      "99   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
      "100  ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.065607    0.0   \n",
      "\n",
      "     youngster      your  \n",
      "0          0.0  0.000000  \n",
      "1          0.0  0.000000  \n",
      "2          0.0  0.000000  \n",
      "3          0.0  0.000000  \n",
      "4          0.0  0.000000  \n",
      "..         ...       ...  \n",
      "96         0.0  0.220035  \n",
      "97         0.0  0.000000  \n",
      "98         0.0  0.000000  \n",
      "99         0.0  0.000000  \n",
      "100        0.0  0.000000  \n",
      "\n",
      "[101 rows x 514 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize text data so model can take it in\n",
    "# TODO make it so english is not hardcoded, flexible for all languages\n",
    "# https://www.justintodata.com/logistic-regression-example-in-python/\n",
    "# https://stackoverflow.com/questions/45961747/append-tfidf-to-pandas-dataframe\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_text = vectorizer.fit_transform(merged_data['Text'])\n",
    "vectorizedTextDF = pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(vectorizedTextDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_useful_cols = merged_data[[\"Age\", \"Gender_ male \"]]\n",
    "X_data = pd.concat([merged_data_useful_cols, vectorizedTextDF], axis=1)\n",
    "\n",
    "# TODO, make binary classifier first? e.g. has or does not have\n",
    "y_data = merged_data[[\"dementia_stage_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender_ male</th>\n",
       "      <th>10</th>\n",
       "      <th>104</th>\n",
       "      <th>303</th>\n",
       "      <th>40</th>\n",
       "      <th>about</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>active</th>\n",
       "      <th>...</th>\n",
       "      <th>with</th>\n",
       "      <th>woman</th>\n",
       "      <th>working</th>\n",
       "      <th>yard</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youngster</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Gender_ male    10  104  303   40    about  acting  action  active  \\\n",
       "0     74              1  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "1     62              0  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "2     69              0  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "3     71              0  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "4     67              1  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "..   ...            ...  ...  ...  ...  ...      ...     ...     ...     ...   \n",
       "96    58              1  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "97    72              1  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "98    68              0  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "99    65              0  0.0  0.0  0.0  0.0  0.17899     0.0     0.0     0.0   \n",
       "100   71              0  0.0  0.0  0.0  0.0  0.00000     0.0     0.0     0.0   \n",
       "\n",
       "     ...      with  woman   working  yard  year  yet       you  young  \\\n",
       "0    ...  0.208362    0.0  0.000000   0.0   0.0  0.0  0.164697    0.0   \n",
       "1    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
       "2    ...  0.274953    0.0  0.000000   0.0   0.0  0.0  0.048296    0.0   \n",
       "3    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.100045    0.0   \n",
       "4    ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
       "..   ...       ...    ...       ...   ...   ...  ...       ...    ...   \n",
       "96   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
       "97   ...  0.000000    0.0  0.239497   0.0   0.0  0.0  0.241061    0.0   \n",
       "98   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.159917    0.0   \n",
       "99   ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.000000    0.0   \n",
       "100  ...  0.000000    0.0  0.000000   0.0   0.0  0.0  0.065607    0.0   \n",
       "\n",
       "     youngster      your  \n",
       "0          0.0  0.000000  \n",
       "1          0.0  0.000000  \n",
       "2          0.0  0.000000  \n",
       "3          0.0  0.000000  \n",
       "4          0.0  0.000000  \n",
       "..         ...       ...  \n",
       "96         0.0  0.220035  \n",
       "97         0.0  0.000000  \n",
       "98         0.0  0.000000  \n",
       "99         0.0  0.000000  \n",
       "100        0.0  0.000000  \n",
       "\n",
       "[101 rows x 516 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dementia_stage_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dementia_stage_num\n",
       "0                     0\n",
       "1                     0\n",
       "2                     0\n",
       "3                     0\n",
       "4                     0\n",
       "..                  ...\n",
       "96                    2\n",
       "97                    1\n",
       "98                    2\n",
       "99                    2\n",
       "100                   2\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set is length: 80\n",
      "Test data set is length: 21\n"
     ]
    }
   ],
   "source": [
    "# dividing X, y into train and test data \n",
    "# 80/20 training/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2)\n",
    "print(\"Training data set is length: \" +str(len(y_train)))\n",
    "print(\"Test data set is length: \" +str(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and predict on Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all parameters not specified are set to their defaults\n",
    "# TODO change max_iter value to higher one?\n",
    "logisticRegr = LogisticRegression(max_iter = 10000)\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_values = {\n",
    "                'penalty' : ['l1', 'l2'],\n",
    "                'C': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]                \n",
    "           }\n",
    "\n",
    "# Try out the different n_estimators parameters\n",
    "grid_search_cv = GridSearchCV(estimator = logisticRegr, param_grid = param_values, cv = None, scoring = 'accuracy') \n",
    "# A value of 'None' for cv causes this method to evaluate performance by 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.5, best penalty: l2\n"
     ]
    }
   ],
   "source": [
    "grid_search_cv.fit(X_train, y_train)\n",
    "\n",
    "best_lr_C_value = grid_search_cv.best_params_['C']\n",
    "best_penalty_value = grid_search_cv.best_params_['penalty']\n",
    "\n",
    "print(\"Best C: \" + str(best_lr_C_value) + \", best penalty: \" + str(best_penalty_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64        15\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.20      0.17      0.18         6\n",
      "\n",
      "    accuracy                           0.48        21\n",
      "   macro avg       0.30      0.26      0.27        21\n",
      "weighted avg       0.55      0.48      0.51        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a model with the best n_estimators value\n",
    "lr_model = LogisticRegression(max_iter = 10000, penalty=best_penalty_value,C=best_lr_C_value)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(classification_report(lr_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show confusion matrix\n",
    "plot_confusion_matrix(lr_model, X_test, y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM multi class classification based on stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be same data\n",
    "# 'OVO' gets better results for some reason\n",
    "# Linear kernel gets better results for some reason\n",
    "clf = SVC(decision_function_shape='ovo')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_values = {\n",
    "                'C': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
    "                'kernel' : ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "           }\n",
    "\n",
    "# Try out the different n_estimators parameters\n",
    "grid_search_cv = GridSearchCV(estimator = clf, param_grid = param_values, cv = None, scoring = 'accuracy') \n",
    "# A value of 'None' for cv causes this method to evaluate performance by 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(X_train, y_train)\n",
    "\n",
    "best_svm_C_value = grid_search_cv.best_params_['C']\n",
    "best_kernel_value = grid_search_cv.best_params_['kernel']\n",
    "\n",
    "print(\"Best C: \" + str(best_svm_C_value) + \", best kernel: \" + str(best_kernel_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with the best n_estimators value\n",
    "svm_model = SVC(C=best_svm_C_value, decision_function_shape='ovo', kernel=best_kernel_value);\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(classification_report(svm_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show confusion matrix\n",
    "plot_confusion_matrix(svm_model, X_test, y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RF classifier \n",
    "random_forest = RandomForestClassifier()   \n",
    "  \n",
    "# Training the model on the training dataset \n",
    "# fit function is used to train the model using the training sets as parameters \n",
    "random_forest.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_values = {'n_estimators': [10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "\n",
    "# Try out the different n_estimators parameters\n",
    "grid_search_cv = GridSearchCV(estimator = random_forest, param_grid = n_estimators_values, cv = None, scoring = 'accuracy') \n",
    "# A value of 'None' for cv causes this method to evaluate performance by 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(X_train, y_train)\n",
    "\n",
    "best_n_estimators = grid_search_cv.best_params_['n_estimators']\n",
    "best_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with the best n_estimators value\n",
    "rf_model = RandomForestClassifier(n_estimators = best_n_estimators);\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_predict_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(classification_report(rf_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using metrics module for accuracy calculation \n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(y_test, rf_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show confusion matrix\n",
    "plot_confusion_matrix(rf_model, X_test, y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default model to track the best model\n",
    "best_model =  MLPClassifier(hidden_layer_sizes = (1,), max_iter = 1000);\n",
    "best_model.fit(X_train, y_train)\n",
    "best_score = best_model.score(X_test, y_test)\n",
    "best_hidden_layer_size = (1,)\n",
    "\n",
    "# Create a neural net model with one hidden layer, iterating through hidden layer sizes\n",
    "for hidden_layer_size in range(1, 100):\n",
    "    model = MLPClassifier(hidden_layer_sizes = (hidden_layer_size,), max_iter = 1000);\n",
    "    model.fit(X_train, y_train)\n",
    "    model_score = model.score(X_test, y_test)\n",
    "    \n",
    "    if model_score > best_score:\n",
    "        best_model = model;\n",
    "        best_score = model_score\n",
    "        best_hidden_layer_size = (hidden_layer_size, )\n",
    "    \n",
    "# Print the classification report for the best NN model that we have come up with\n",
    "mlp_model = best_model\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "print(\"The hidden layer size of our best-performing neural network in this experiment is: \" + str(best_hidden_layer_size))\n",
    "print(classification_report(mlp_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using metrics module for accuracy calculation \n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(y_test, mlp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show confusion matrix\n",
    "plot_confusion_matrix(mlp_model, X_test, y_test)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked():\n",
    "    estimators = [\n",
    "        ('rf',  RandomForestClassifier(n_estimators=best_n_estimators)),\n",
    "        ('svm', SVC(C = best_svm_C_value, decision_function_shape='ovo', kernel=best_kernel_value)),\n",
    "        ('nn', MLPClassifier(hidden_layer_sizes = best_hidden_layer_size, max_iter = 1000)),\n",
    "        ('lr', LogisticRegression(C=best_lr_C_value, penalty=best_penalty_value))\n",
    "    ]\n",
    "    return StackingClassifier(estimators=estimators)\n",
    "\n",
    "clf = create_stacked()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using metrics module for accuracy calculation \n",
    "print(\"ACCURACY OF THE MODEL: \", accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show confusion matrix\n",
    "plot_confusion_matrix(clf, X_test, y_test)\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
